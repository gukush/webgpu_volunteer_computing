./test_frameworks.sh
#!/usr/bin/env bash
# test_frameworks.sh - submit one tiny job per framework via submit-task.mjs
set -euo pipefail

SUBMIT_TASK=${SUBMIT_TASK:-"./server/scripts/submit-task.mjs"}
SERVER_URL=${SERVER_URL:-"https://localhost:3000"}
LABEL_PREFIX=${LABEL_PREFIX:-"smoke"}
INCLUDE_CUDA=${INCLUDE_CUDA:-"0"}

WG_COUNTS="1,1,1"
OUT_BYTES=$((64 * 4))

run_compute() {
  local fw="$1"; shift
  local kfile="$1"; shift
  local label="${LABEL_PREFIX}-${fw}"
  echo "==> Submitting ${fw} job (${kfile})"
  node "$SUBMIT_TASK" compute \
    --server "$SERVER_URL" \
    --framework "$fw" \
    --kernel "$kfile" \
    --workgroups "$WG_COUNTS" \
    --output-size "$OUT_BYTES" \
    --label "$label" "$@"
}

AVAILABLE=$(node "$SUBMIT_TASK" frameworks 2>/dev/null || echo '["webgpu","webgl","opencl","vulkan"]')
has() { echo "$AVAILABLE" | grep -qi "$1"; }

if has webgpu; then run_compute webgpu "$KDIR/webgpu_smoke.wgsl"; fi
if has webgl; then
  node "$SUBMIT_TASK" compute-advanced \
    --server "$SERVER_URL" \
    --framework webgl \
    --kernel "$KDIR/webgl_smoke.glsl" \
    --label "${LABEL_PREFIX}-webgl" \
    --metadata '{"width":8,"height":8}'
fi
if has opencl; then run_compute opencl "$KDIR/opencl_smoke.cl"; fi
if has vulkan; then run_compute vulkan "$KDIR/vulkan_smoke.comp"; fi
if [[ "$INCLUDE_CUDA" == "1" ]] && has cuda; then run_compute cuda "$KDIR/cuda_smoke.cu"; fi

echo "All submissions attempted."
./main.cpp
#if defined(HAVE_CUDA) && (defined(CLIENT_CUDA) || defined(CLIENT_UNIVERSAL))
  #include "cuda/cuda_executor.hpp"
#endif
#if defined(HAVE_OPENCL) && (defined(CLIENT_OPENCL) || defined(CLIENT_UNIVERSAL))
  #include "opencl/opencl_executor.hpp"
#endif
#if defined(HAVE_VULKAN) && (defined(CLIENT_VULKAN) || defined(CLIENT_UNIVERSAL))
  #include "vulkan/vulkan_executor.hpp"
#endif

#include <iostream>
#include <fstream>
#include <memory>
#include <csignal>
#include <nlohmann/json.hpp>
using nlohmann::json;

std::unique_ptr<FrameworkClient> globalClient;

void signalHandler(int signum) {
    std::cout << "\nReceived signal " << signum << ", shutting down..." << std::endl;
    if (globalClient) globalClient->disconnect();
    std::exit(signum);
}

static void printUsage(const char* programName) {
    std::cout << "Usage: " << programName << " <framework> [options]\n"
              << "Frameworks: cuda, opencl, vulkan\n"
              << "Options:\n"
              << "  --url <ws://localhost:3000>  Server URL\n"
              << "  --device <0>                 Device ID\n"
              << "  --config <file.json>         Configuration file\n"
              << std::endl;
}

int main(int argc, char* argv[]) {
    if (argc < 2) { printUsage(argv[0]); return 1; }

    std::string framework = argv[1];
    std::string serverUrl = "wss://localhost:3000";
    int deviceId = 0;
    std::string configFile;

    for (int i = 2; i + 1 < argc; i += 2) {
        std::string arg = argv[i], value = argv[i + 1];
        if (arg == "--url") serverUrl = value;
        else if (arg == "--device") deviceId = std::stoi(value);
        else if (arg == "--config") configFile = value;
    }

    // Create framework executor
    std::unique_ptr<IFrameworkExecutor> executor;

    if (framework == "cuda") {
    #if defined(HAVE_CUDA)
        executor = std::make_unique<CudaExecutor>(deviceId);
    #else
        std::cerr << "This binary was built without CUDA support.\n";
        return 1;
    #endif
    }
    else if (framework == "opencl") {
    #if defined(HAVE_OPENCL)
        executor = std::make_unique<OpenCLExecutor>();
    #else
        std::cerr << "This binary was built without OpenCL support.\n";
        return 1;
    #endif
    }
    else if (framework == "vulkan") {
    #if defined(HAVE_VULKAN)
        executor = std::make_unique<VulkanExecutor>();
    #else
        std::cerr << "This binary was built without Vulkan support.\n";
        return 1;
    #endif
    }
    else {
        std::cerr << "Unknown framework: " << framework << "\n";
        printUsage(argv[0]);
        return 1;
    }

    if (!executor) { std::cerr << "No executor created (internal error).\n"; return 1; }

    // Load config
    json config;
    if (!configFile.empty()) {
        std::ifstream configStream(configFile);
        if (configStream.is_open()) configStream >> config;
    }
    config["deviceId"] = deviceId;

    if (!executor->initialize(config)) {
        std::cerr << "Failed to initialize " << framework << " executor\n";
        return 1;
    }

    // Client
    globalClient = std::make_unique<FrameworkClient>(std::move(executor));
    std::signal(SIGINT,  signalHandler);
    std::signal(SIGTERM, signalHandler);

    std::cout << "Connecting to server: " << serverUrl << std::endl;
    if (!globalClient->connect(serverUrl)) { std::cerr << "Failed to connect to server\n"; return 1; }

    std::cout << "Connected! Starting event loop..." << std::endl;
    globalClient->run();
    return 0;
}./cuda/cuda_executor.hpp
#pragma once
#include "../common/framework_client.hpp"
#include <cuda_runtime.h>
#include <cuda.h>
#include <nvrtc.h>
#include <vector>
#include <memory>

class CudaExecutor : public IFrameworkExecutor {
private:
    int deviceId = 0;
    cudaDeviceProp deviceProps;
    CUcontext context = nullptr;
    bool initialized = false;

    struct CompiledKernel {
        CUmodule module = nullptr;
        CUfunction function = nullptr;
        std::string ptx;
    };

    std::map<std::string, CompiledKernel> kernelCache;

    bool compileKernel(const std::string& source, const std::string& entryPoint,
                      const json& compileOpts, CompiledKernel& result);

public:
    CudaExecutor(int deviceId = 0);
    ~CudaExecutor() override;

    bool initialize(const json& config = {}) override;
    void cleanup() override;
    TaskResult executeTask(const TaskData& task) override;
    std::string getFrameworkName() const override { return "cuda"; }
    json getCapabilities() const override;
};
./cuda/cuda_executor.cpp
#include "cuda_executor.hpp"
#include <cuda.h>
#include <iostream>
#include <chrono>

CudaExecutor::CudaExecutor(int devId) : deviceId(devId) {}

CudaExecutor::~CudaExecutor() {
    cleanup();
}

bool CudaExecutor::initialize(const json& config) {
    if (initialized) return true;
    
    // Initialize CUDA driver API
    CUresult result = cuInit(0);
    if (result != CUDA_SUCCESS) {
        std::cerr << "Failed to initialize CUDA driver API" << std::endl;
        return false;
    }
    
    // Get device count
    int deviceCount;
    if (cudaGetDeviceCount(&deviceCount) != cudaSuccess || deviceCount == 0) {
        std::cerr << "No CUDA devices found" << std::endl;
        return false;
    }
    
    if (deviceId >= deviceCount) {
        std::cerr << "Invalid device ID: " << deviceId << std::endl;
        return false;
    }
    
    // Set device and get properties
    if (cudaSetDevice(deviceId) != cudaSuccess) {
        std::cerr << "Failed to set CUDA device " << deviceId << std::endl;
        return false;
    }
    
    if (cudaGetDeviceProperties(&deviceProps, deviceId) != cudaSuccess) {
        std::cerr << "Failed to get device properties" << std::endl;
        return false;
    }
    
    // Create CUDA context
    CUdevice device;
    if (cuDeviceGet(&device, deviceId) != CUDA_SUCCESS) {
        std::cerr << "Failed to get CUDA device" << std::endl;
        return false;
    }
    
    if (cuCtxCreate(&context, 0, device) != CUDA_SUCCESS) {
        std::cerr << "Failed to create CUDA context" << std::endl;
        return false;
    }
    
    initialized = true;
    std::cout << "CUDA initialized on device " << deviceId 
              << " (" << deviceProps.name << ")" << std::endl;
    return true;
}

void CudaExecutor::cleanup() {
    if (!initialized) return;
    
    // Cleanup cached kernels
    for (auto& [key, kernel] : kernelCache) {
        if (kernel.module) {
            cuModuleUnload(kernel.module);
        }
    }
    kernelCache.clear();
    
    // Cleanup context
    if (context) {
        cuCtxDestroy(context);
        context = nullptr;
    }
    
    initialized = false;
}

bool CudaExecutor::compileKernel(const std::string& source, const std::string& entryPoint,
                                const json& compileOpts, CompiledKernel& result) {
    // Create NVRTC program
    nvrtcProgram prog;
    nvrtcResult res = nvrtcCreateProgram(&prog, source.c_str(), nullptr, 0, nullptr, nullptr);
    if (res != NVRTC_SUCCESS) {
        std::cerr << "Failed to create NVRTC program: " << nvrtcGetErrorString(res) << std::endl;
        return false;
    }
    
    // Prepare compilation options
    std::vector<const char*> opts;
    std::string computeCapability = "--gpu-architecture=compute_" + 
        std::to_string(deviceProps.major) + std::to_string(deviceProps.minor);
    opts.push_back(computeCapability.c_str());
    
    // Add user-specified options
    std::vector<std::string> optStrings;
    if (compileOpts.contains("optimization")) {
        optStrings.push_back(compileOpts["optimization"].get<std::string>());
        opts.push_back(optStrings.back().c_str());
    }
    
    // Compile
    res = nvrtcCompileProgram(prog, opts.size(), opts.data());
    if (res != NVRTC_SUCCESS) {
        size_t logSize;
        nvrtcGetProgramLogSize(prog, &logSize);
        std::vector<char> log(logSize);
        nvrtcGetProgramLog(prog, log.data());
        std::cerr << "CUDA compilation failed:\n" << log.data() << std::endl;
        nvrtcDestroyProgram(&prog);
        return false;
    }
    
    // Get PTX
    size_t ptxSize;
    nvrtcGetPTXSize(prog, &ptxSize);
    result.ptx.resize(ptxSize);
    nvrtcGetPTX(prog, result.ptx.data());
    nvrtcDestroyProgram(&prog);
    
    // Load module
    CUresult cuRes = cuModuleLoadDataEx(&result.module, result.ptx.c_str(), 0, 0, 0);
    if (cuRes != CUDA_SUCCESS) {
        std::cerr << "Failed to load CUDA module" << std::endl;
        return false;
    }
    
    // Get function
    cuRes = cuModuleGetFunction(&result.function, result.module, entryPoint.c_str());
    if (cuRes != CUDA_SUCCESS) {
        std::cerr << "Failed to get CUDA function: " << entryPoint << std::endl;
        cuModuleUnload(result.module);
        return false;
    }
    
    return true;
}

TaskResult CudaExecutor::executeTask(const TaskData& task) {
    TaskResult result;
    auto startTime = std::chrono::high_resolution_clock::now();
    
    if (!initialized) {
        result.success = false;
        result.errorMessage = "CUDA not initialized";
        return result;
    }
    
    try {
        // Set context
        cuCtxSetCurrent(context);
        
        // Compile kernel if not cached
        std::string cacheKey = task.kernel + "|" + task.entry;
        auto it = kernelCache.find(cacheKey);
        CompiledKernel* kernel;
        
        if (it == kernelCache.end()) {
            CompiledKernel newKernel;
            if (!compileKernel(task.kernel, task.entry, task.compilationOptions, newKernel)) {
                result.success = false;
                result.errorMessage = "Kernel compilation failed";
                return result;
            }
            kernelCache[cacheKey] = std::move(newKernel);
            kernel = &kernelCache[cacheKey];
        } else {
            kernel = &it->second;
        }
        
        // Allocate GPU memory
        CUdeviceptr d_input = 0, d_output = 0;
        
        if (!task.inputData.empty()) {
            if (cuMemAlloc(&d_input, task.inputData.size()) != CUDA_SUCCESS) {
                result.success = false;
                result.errorMessage = "Failed to allocate input memory";
                return result;
            }
            
            if (cuMemcpyHtoD(d_input, task.inputData.data(), task.inputData.size()) != CUDA_SUCCESS) {
                cuMemFree(d_input);
                result.success = false;
                result.errorMessage = "Failed to copy input data";
                return result;
            }
        }
        
        if (cuMemAlloc(&d_output, task.outputSize) != CUDA_SUCCESS) {
            if (d_input) cuMemFree(d_input);
            result.success = false;
            result.errorMessage = "Failed to allocate output memory";
            return result;
        }
        
        // Setup kernel parameters
        void* args[] = { &d_input, &d_output };
        
        // Launch kernel
        CUresult launchResult = cuLaunchKernel(
            kernel->function,
            task.workgroupCount[0], task.workgroupCount[1], task.workgroupCount[2],  // grid
            256, 1, 1,  // block (could be made configurable)
            0, 0,  // shared mem, stream
            args, nullptr
        );
        
        if (launchResult != CUDA_SUCCESS) {
            if (d_input) cuMemFree(d_input);
            cuMemFree(d_output);
            result.success = false;
            result.errorMessage = "Kernel launch failed";
            return result;
        }
        
        // Wait for completion
        if (cuCtxSynchronize() != CUDA_SUCCESS) {
            if (d_input) cuMemFree(d_input);
            cuMemFree(d_output);
            result.success = false;
            result.errorMessage = "Kernel execution failed";
            return result;
        }
        
        // Copy result back
        result.outputData.resize(task.outputSize);
        if (cuMemcpyDtoH(result.outputData.data(), d_output, task.outputSize) != CUDA_SUCCESS) {
            if (d_input) cuMemFree(d_input);
            cuMemFree(d_output);
            result.success = false;
            result.errorMessage = "Failed to copy output data";
            return result;
        }
        
        // Cleanup
        if (d_input) cuMemFree(d_input);
        cuMemFree(d_output);
        
        auto endTime = std::chrono::high_resolution_clock::now();
        result.processingTime = std::chrono::duration<double, std::milli>(endTime - startTime).count();
        result.success = true;
        
    } catch (const std::exception& e) {
        result.success = false;
        result.errorMessage = std::string("Exception: ") + e.what();
    }
    
    return result;
}

json CudaExecutor::getCapabilities() const {
    json caps;
    caps["framework"] = "cuda";
    caps["initialized"] = initialized;
    
    if (initialized) {
        caps["device"] = {
            {"id", deviceId},
            {"name", deviceProps.name},
            {"computeCapability", {deviceProps.major, deviceProps.minor}},
            {"globalMemory", deviceProps.totalGlobalMem},
            {"maxThreadsPerBlock", deviceProps.maxThreadsPerBlock},
            {"multiProcessorCount", deviceProps.multiProcessorCount}
        };
    }
    
    return caps;
}
./common/websocket_client.cpp
#include "websocket_client.hpp"
#include <iostream>
#include <boost/beast/core.hpp>
#include <boost/beast/websocket.hpp>
#include <boost/beast/websocket/ssl.hpp>
#include <boost/asio/ssl.hpp>
#include <boost/url.hpp>

namespace beast = boost::beast;
namespace websocket = beast::websocket;
namespace net = boost::asio;
namespace ssl = boost::asio::ssl;
using tcp = boost::asio::ip::tcp;

WebSocketClient::WebSocketClient() {
    // Configure SSL context to accept self-signed certificates
    ctx.set_verify_mode(ssl::verify_none);
    ctx.set_options(ssl::context::default_workarounds |
                   ssl::context::no_sslv2 |
                   ssl::context::no_sslv3 |
                   ssl::context::single_dh_use);
}

WebSocketClient::~WebSocketClient() {
    disconnect();
}

bool WebSocketClient::connect(const std::string& host, const std::string& port, const std::string& target) {
    try {
        // Resolve hostname
        auto const results = resolver.resolve(host, port);

        // Get the underlying socket
        auto& socket = beast::get_lowest_layer(ws);

        // Make the connection on the IP address we get from a lookup
        auto ep = net::connect(socket, results);

        // Update the host string for SNI
        std::string hostWithPort = host + ':' + std::to_string(ep.port());

        // Set SNI Hostname (many hosts need this to handshake successfully)
        if (!SSL_set_tlsext_host_name(ws.next_layer().native_handle(), host.c_str())) {
            beast::error_code ec{static_cast<int>(::ERR_get_error()), net::error::get_ssl_category()};
            throw beast::system_error{ec};
        }

        // Perform the SSL handshake
        ws.next_layer().handshake(ssl::stream_base::client);

        // Set a decorator to change the User-Agent of the handshake
        ws.set_option(websocket::stream_base::decorator(
            [](websocket::request_type& req) {
                req.set(beast::http::field::user_agent, "MultiFramework-Native-Client/1.0");
            }));

        // Perform the websocket handshake
        ws.handshake(hostWithPort, target);

        // Start the event loop in a separate thread
        shouldStop = false;
        ioThread = std::thread(&WebSocketClient::runEventLoop, this);

        if (onConnected) {
            onConnected();
        }

        return true;

    } catch (std::exception const& e) {
        std::cerr << "WebSocket connection error: " << e.what() << std::endl;
        return false;
    }
}

void WebSocketClient::disconnect() {
    shouldStop = true;

    if (ws.is_open()) {
        try {
            ws.close(websocket::close_code::normal);
        } catch (...) {
            // Ignore errors during close
        }
    }

    if (ioThread.joinable()) {
        ioThread.join();
    }

    if (onDisconnected) {
        onDisconnected();
    }
}

void WebSocketClient::send(const std::string& message) {
    if (!ws.is_open()) {
        std::cerr << "WebSocket not connected, cannot send message" << std::endl;
        return;
    }

    try {
        ws.write(net::buffer(message));
    } catch (std::exception const& e) {
        std::cerr << "WebSocket send error: " << e.what() << std::endl;
    }
}

void WebSocketClient::runEventLoop() {
    beast::flat_buffer buffer;

    while (!shouldStop && ws.is_open()) {
        try {
            // Read a message
            ws.read(buffer);

            // Convert to string
            std::string message = beast::buffers_to_string(buffer.data());
            buffer.clear();

            // Call message handler
            if (onMessage) {
                onMessage(message);
            }

        } catch (beast::system_error const& se) {
            if (se.code() != websocket::error::closed) {
                std::cerr << "WebSocket read error: " << se.code().message() << std::endl;
            }
            break;
        } catch (std::exception const& e) {
            std::cerr << "WebSocket event loop error: " << e.what() << std::endl;
            break;
        }
    }
}

./common/framework_client.cpp
#include "framework_client.hpp"
#include <iostream>
#include <thread>
#include <chrono>
#include <boost/url.hpp>

FrameworkClient::FrameworkClient(std::unique_ptr<IFrameworkExecutor> exec)
    : executor(std::move(exec)) {
    wsClient = std::make_unique<WebSocketClient>();

    // Setup WebSocket event handlers
    wsClient->setOnConnected([this]() { onConnected(); });
    wsClient->setOnDisconnected([this]() { onDisconnected(); });
    wsClient->setOnMessage([this](const std::string& msg) { onMessage(msg); });
}

FrameworkClient::~FrameworkClient() {
    disconnect();
}

bool FrameworkClient::connect(const std::string& url) {
    serverUrl = url;

    try {
        // Parse WebSocket URL
        boost::urls::url parsedUrl(url);
        std::string scheme = parsedUrl.scheme();
        std::string host = parsedUrl.host();
        std::string port = parsedUrl.has_port() ? parsedUrl.port() : (scheme == "wss" ? "443" : "80");
        std::string target = parsedUrl.path();

        if (target.empty()) {
            target = "/";
        }

        // Add socket.io path if not present
        if (target.find("socket.io") == std::string::npos) {
            target += "socket.io/?EIO=4&transport=websocket";
        }

        return wsClient->connect(host, port, target);

    } catch (std::exception const& e) {
        std::cerr << "Failed to parse URL: " << e.what() << std::endl;
        return false;
    }
}

void FrameworkClient::disconnect() {
    if (wsClient) {
        wsClient->disconnect();
    }
    connected = false;
}

void FrameworkClient::run() {
    // Keep the main thread alive while connected
    while (connected) {
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

void FrameworkClient::onConnected() {
    connected = true;
    std::cout << "Connected to server" << std::endl;

    // Send initial registration
    json registerMsg = {
        {"type", "client:join"},
        {"data", {
            {"gpuInfo", executor->getCapabilities()},
            {"hasWebGPU", false},
            {"supportedFrameworks", {executor->getFrameworkName()}},
            {"clientType", "native"}
        }}
    };

    // Socket.io message format: "42" + JSON
    std::string message = "42" + registerMsg.dump();
    wsClient->send(message);
}

void FrameworkClient::onDisconnected() {
    connected = false;
    std::cout << "Disconnected from server" << std::endl;
}

void FrameworkClient::onMessage(const std::string& message) {
    try {
        // Parse Socket.io message format
        if (message.length() < 2 || message.substr(0, 2) != "42") {
            return; // Not a data message
        }

        json data = json::parse(message.substr(2));

        if (!data.is_array() || data.size() < 2) {
            return;
        }

        std::string eventType = data[0];
        json eventData = data[1];

        std::cout << "Received event: " << eventType << std::endl;

        if (eventType == "register") {
            clientId = eventData["clientId"];
            std::cout << "Registered with client ID: " << clientId << std::endl;

        } else if (eventType == "workload:new") {
            handleWorkloadAssignment(eventData);

        } else if (eventType == "workload:chunk_assign") {
            handleChunkAssignment(eventData);

        } else if (eventType == "task:assign") {
            // Matrix multiplication task - could implement if needed
            std::cout << "Matrix task assignment not implemented for native clients" << std::endl;
        }

    } catch (json::exception const& e) {
        std::cerr << "JSON parsing error: " << e.what() << std::endl;
    } catch (std::exception const& e) {
        std::cerr << "Message handling error: " << e.what() << std::endl;
    }
}

std::vector<std::vector<uint8_t>> FrameworkClient::decodeInputs(const json& data) {
    std::vector<std::vector<uint8_t>> inputs;

    // NEW: Check for multi-input format first
    if (data.contains("inputs") && data["inputs"].is_array()) {
        // Multi-input format: ["base64_1", "base64_2", ...]
        for (const auto& inputBase64 : data["inputs"]) {
            if (inputBase64.is_string() && !inputBase64.get<std::string>().empty()) {
                inputs.push_back(base64_decode(inputBase64.get<std::string>()));
            } else {
                inputs.push_back(std::vector<uint8_t>()); // Empty input
            }
        }
    } else if (data.contains("input") && !data["input"].is_null()) {
        // Legacy single input format
        std::string inputBase64 = data["input"];
        if (!inputBase64.empty()) {
            inputs.push_back(base64_decode(inputBase64));
        }
    } else if (data.contains("inputData") && !data["inputData"].is_null()) {
        // Chunk format single input
        std::string inputBase64 = data["inputData"];
        if (!inputBase64.empty()) {
            inputs.push_back(base64_decode(inputBase64));
        }
    }

    return inputs;
}

TaskData FrameworkClient::parseTaskData(const json& data, bool isChunk) {
    TaskData task;

    if (isChunk) {
        task.id = data["chunkId"];
        task.parentId = data["parentId"];
        task.chunkId = data["chunkId"];
        task.chunkOrderIndex = data.value("chunkOrderIndex", -1);
        task.isChunk = true;
    } else {
        task.id = data["id"];
    }

    task.framework = data["framework"];
    task.kernel = data.contains("kernel") ? data["kernel"].get<std::string>() : data["wgsl"].get<std::string>();
    task.entry = data["entry"];
    task.bindLayout = data["bindLayout"];
    task.compilationOptions = data.value("compilationOptions", json::object());

    if (isChunk) {
        task.chunkUniforms = data.value("chunkUniforms", json::object());
    }

    // Parse workgroup count
    if (data.contains("workgroupCount") && data["workgroupCount"].is_array()) {
        task.workgroupCount = data["workgroupCount"].get<std::vector<int>>();
    } else {
        task.workgroupCount = {1, 1, 1}; // Default
    }

    // NEW: Parse multi-input data
    task.inputData = decodeInputs(data);

    // NEW: Parse multi-output sizes
    if (data.contains("outputSizes") && data["outputSizes"].is_array()) {
        task.outputSizes = data["outputSizes"].get<std::vector<size_t>>();
    } else if (data.contains("outputSize")) {
        // Legacy single output
        task.outputSizes = {data["outputSize"].get<size_t>()};
    } else {
        // Default output size
        task.outputSizes = {1024};
    }

    // Set legacy fields for backward compatibility
    if (!task.inputData.empty()) {
        task.legacyInputData = task.inputData[0];
    }
    if (!task.outputSizes.empty()) {
        task.legacyOutputSize = task.outputSizes[0];
    }

    return task;
}

void FrameworkClient::handleWorkloadAssignment(const json& data) {
    if (busy) {
        // Send busy response
        json response = {
            {"type", "workload:busy"},
            {"data", {
                {"id", data["id"]},
                {"reason", "busy"}
            }}
        };
        wsClient->send("42" + response.dump());
        return;
    }

    try {
        TaskData task = parseTaskData(data, false);

        // Check framework compatibility
        if (task.framework != executor->getFrameworkName()) {
            json response = {
                {"type", "workload:error"},
                {"data", {
                    {"id", task.id},
                    {"message", "Framework mismatch: expected " + executor->getFrameworkName() + ", got " + task.framework}
                }}
            };
            wsClient->send("42" + response.dump());
            return;
        }

        busy = true;
        activeTasks[task.id] = task;

        std::cout << "Executing " << task.framework << " workload: " << task.id
                  << " (" << task.getInputCount() << " inputs, " << task.getOutputCount() << " outputs)" << std::endl;

        // Execute in separate thread to avoid blocking
        std::thread([this, task]() {
            TaskResult result = executor->executeTask(task);
            sendResult(task, result);

            busy = false;
            activeTasks.erase(task.id);
        }).detach();

    } catch (std::exception const& e) {
        std::cerr << "Workload assignment error: " << e.what() << std::endl;

        json response = {
            {"type", "workload:error"},
            {"data", {
                {"id", data["id"]},
                {"message", std::string("Assignment error: ") + e.what()}
            }}
        };
        wsClient->send("42" + response.dump());
    }
}

void FrameworkClient::handleChunkAssignment(const json& data) {
    if (busy) {
        json response = {
            {"type", "workload:chunk_error"},
            {"data", {
                {"parentId", data["parentId"]},
                {"chunkId", data["chunkId"]},
                {"message", "busy"}
            }}
        };
        wsClient->send("42" + response.dump());
        return;
    }

    try {
        TaskData task = parseTaskData(data, true);

        busy = true;
        activeTasks[task.id] = task;

        std::cout << "Executing chunk: " << task.chunkId
                  << " (" << task.getInputCount() << " inputs, " << task.getOutputCount() << " outputs)" << std::endl;

        // Execute in separate thread
        std::thread([this, task]() {
            TaskResult result = executor->executeTask(task);

            if (result.success) {
                json responseData = {
                    {"parentId", task.parentId},
                    {"chunkId", task.chunkId},
                    {"chunkOrderIndex", task.chunkOrderIndex},
                    {"processingTime", result.processingTime}
                };

                // NEW: Send multi-output results
                if (result.hasMultipleOutputs()) {
                    json resultsArray = json::array();
                    for (const auto& output : result.outputData) {
                        resultsArray.push_back(base64_encode(output));
                    }
                    responseData["results"] = resultsArray;

                    // Also send single result for backward compatibility
                    if (!result.outputData.empty()) {
                        responseData["result"] = base64_encode(result.outputData[0]);
                    }
                } else {
                    // Single output (legacy format)
                    if (!result.outputData.empty()) {
                        responseData["result"] = base64_encode(result.outputData[0]);
                    } else if (!result.legacyOutputData.empty()) {
                        responseData["result"] = base64_encode(result.legacyOutputData);
                    }
                }

                json response = {
                    {"type", "workload:chunk_done"},
                    {"data", responseData}
                };
                wsClient->send("42" + response.dump());
            } else {
                json response = {
                    {"type", "workload:chunk_error"},
                    {"data", {
                        {"parentId", task.parentId},
                        {"chunkId", task.chunkId},
                        {"message", result.errorMessage}
                    }}
                };
                wsClient->send("42" + response.dump());
            }

            busy = false;
            activeTasks.erase(task.id);
        }).detach();

    } catch (std::exception const& e) {
        std::cerr << "Chunk assignment error: " << e.what() << std::endl;

        json response = {
            {"type", "workload:chunk_error"},
            {"data", {
                {"parentId", data["parentId"]},
                {"chunkId", data["chunkId"]},
                {"message", std::string("Assignment error: ") + e.what()}
            }}
        };
        wsClient->send("42" + response.dump());
    }
}

void FrameworkClient::sendResult(const TaskData& task, const TaskResult& result) {
    if (result.success) {
        json responseData = {
            {"id", task.id},
            {"processingTime", result.processingTime}
        };

        // NEW: Send multi-output results
        if (result.hasMultipleOutputs()) {
            json resultsArray = json::array();
            for (const auto& output : result.outputData) {
                resultsArray.push_back(base64_encode(output));
            }
            responseData["results"] = resultsArray;

            // Also send single result for backward compatibility
            if (!result.outputData.empty()) {
                responseData["result"] = base64_encode(result.outputData[0]);
            }
        } else {
            // Single output (legacy format)
            if (!result.outputData.empty()) {
                responseData["result"] = base64_encode(result.outputData[0]);
            } else if (!result.legacyOutputData.empty()) {
                responseData["result"] = base64_encode(result.legacyOutputData);
            }
        }

        json response = {
            {"type", "workload:done"},
            {"data", responseData}
        };
        wsClient->send("42" + response.dump());

        std::cout << "Workload " << task.id << " completed in "
                  << result.processingTime << "ms (" << result.getOutputCount() << " outputs)" << std::endl;
    } else {
        sendError(task, result.errorMessage);
    }
}

void FrameworkClient::sendError(const TaskData& task, const std::string& error) {
    json response = {
        {"type", "workload:error"},
        {"data", {
            {"id", task.id},
            {"message", error}
        }}
    };
    wsClient->send("42" + response.dump());
    std::cerr << "Workload " << task.id << " failed: " << error << std::endl;
}./common/framework_client.hpp
// native-client/common/framework_client.hpp
#pragma once

#include <string>
#include <vector>
#include <memory>
#include <functional>
#include <map>

// Base64 utilities
#include "base64.hpp"
#include <nlohmann/json.hpp>
#include "websocket_client.hpp"

using json = nlohmann::json;

struct TaskData {
    std::string id;
    std::string parentId;
    std::string framework;
    std::string kernel;
    std::string entry;
    std::vector<int> workgroupCount;
    std::string bindLayout;

    // NEW: Multi-input support
    std::vector<std::vector<uint8_t>> inputData; // Multiple inputs
    std::vector<size_t> outputSizes; // Multiple output sizes

    // Legacy single input/output (for backward compatibility)
    std::vector<uint8_t> legacyInputData;
    size_t legacyOutputSize;

    json compilationOptions;
    json chunkUniforms;
    bool isChunk = false;
    std::string chunkId;
    int chunkOrderIndex = -1;

    // Helper methods
    bool hasMultipleInputs() const { return inputData.size() > 1; }
    bool hasMultipleOutputs() const { return outputSizes.size() > 1; }
    size_t getInputCount() const { return std::max(inputData.size(), legacyInputData.empty() ? 0 : 1); }
    size_t getOutputCount() const { return std::max(outputSizes.size(), legacyOutputSize > 0 ? 1 : 0); }
};

struct TaskResult {
    // NEW: Multi-output support
    std::vector<std::vector<uint8_t>> outputData; // Multiple outputs

    // Legacy single output (for backward compatibility)
    std::vector<uint8_t> legacyOutputData;

    double processingTime;
    bool success;
    std::string errorMessage;

    // Helper methods
    bool hasMultipleOutputs() const { return outputData.size() > 1; }
    size_t getOutputCount() const { return std::max(outputData.size(), legacyOutputData.empty() ? 0 : 1); }
};

class IFrameworkExecutor {
public:
    virtual ~IFrameworkExecutor() = default;
    virtual bool initialize(const json& config = {}) = 0;
    virtual void cleanup() = 0;
    virtual TaskResult executeTask(const TaskData& task) = 0;
    virtual std::string getFrameworkName() const = 0;
    virtual json getCapabilities() const = 0;
};

class FrameworkClient {
private:
    std::unique_ptr<WebSocketClient> wsClient;
    std::unique_ptr<IFrameworkExecutor> executor;
    std::string serverUrl;
    std::string clientId;
    bool connected = false;
    bool busy = false;

    // Task tracking
    std::map<std::string, TaskData> activeTasks;

    // Handlers
    void onConnected();
    void onDisconnected();
    void onMessage(const std::string& message);
    void handleWorkloadAssignment(const json& data);
    void handleChunkAssignment(const json& data);
    void sendResult(const TaskData& task, const TaskResult& result);
    void sendError(const TaskData& task, const std::string& error);

    // NEW: Helper methods for multi-input/output
    TaskData parseTaskData(const json& data, bool isChunk = false);
    std::vector<std::vector<uint8_t>> decodeInputs(const json& data);

public:
    FrameworkClient(std::unique_ptr<IFrameworkExecutor> exec);
    ~FrameworkClient();

    bool connect(const std::string& url);
    void disconnect();
    void run(); // Main event loop

    // Status
    bool isConnected() const { return connected; }
    bool isBusy() const { return busy; }
    std::string getClientId() const { return clientId; }
};


// ============================================================================
// CMakeLists.txt for building
// ============================================================================

/*
cmake_minimum_required(VERSION 3.18)
project(MultiFrameworkClient LANGUAGES CXX CUDA)

set(CMAKE_CXX_STANDARD 17)

# Find required packages
find_package(Boost REQUIRED COMPONENTS system)
find_package(OpenSSL REQUIRED)
find_package(PkgConfig REQUIRED)
pkg_check_modules(JSONCPP jsoncpp)

# CUDA
find_package(CUDAToolkit REQUIRED)
enable_language(CUDA)

# OpenCL
find_package(OpenCL REQUIRED)

# Include directories
include_directories(${CMAKE_CURRENT_SOURCE_DIR})

# Create executables for each framework
add_executable(cuda_client
    main.cpp
    common/framework_client.cpp
    common/websocket_client.cpp
    cuda/cuda_executor.cpp
)

target_link_libraries(cuda_client
    ${Boost_LIBRARIES}
    OpenSSL::SSL
    OpenSSL::Crypto
    CUDA::cuda_driver
    CUDA::nvrtc
    pthread
)

add_executable(opencl_client
    main.cpp
    common/framework_client.cpp
    common/websocket_client.cpp
    opencl/opencl_executor.cpp
)

target_link_libraries(opencl_client
    ${Boost_LIBRARIES}
    OpenSSL::SSL
    OpenSSL::Crypto
    OpenCL::OpenCL
    pthread
)
*/./common/base64.cpp
#include "base64.hpp"
#include <algorithm>

static const char base64_chars[] =
    "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    "abcdefghijklmnopqrstuvwxyz"
    "0123456789+/";

static inline bool is_base64(unsigned char c) {
    return (isalnum(c) || (c == '+') || (c == '/'));
}

std::string base64_encode(const std::vector<uint8_t>& data) {
    std::string ret;
    int i = 0;
    int j = 0;
    unsigned char char_array_3[3];
    unsigned char char_array_4[4];
    int len = data.size();
    const unsigned char* bytes_to_encode = data.data();

    while (len--) {
        char_array_3[i++] = *(bytes_to_encode++);
        if (i == 3) {
            char_array_4[0] = (char_array_3[0] & 0xfc) >> 2;
            char_array_4[1] = ((char_array_3[0] & 0x03) << 4) + ((char_array_3[1] & 0xf0) >> 4);
            char_array_4[2] = ((char_array_3[1] & 0x0f) << 2) + ((char_array_3[2] & 0xc0) >> 6);
            char_array_4[3] = char_array_3[2] & 0x3f;

            for(i = 0; (i <4) ; i++)
                ret += base64_chars[char_array_4[i]];
            i = 0;
        }
    }

    if (i) {
        for(j = i; j < 3; j++)
            char_array_3[j] = '\0';

        char_array_4[0] = (char_array_3[0] & 0xfc) >> 2;
        char_array_4[1] = ((char_array_3[0] & 0x03) << 4) + ((char_array_3[1] & 0xf0) >> 4);
        char_array_4[2] = ((char_array_3[1] & 0x0f) << 2) + ((char_array_3[2] & 0xc0) >> 6);
        char_array_4[3] = char_array_3[2] & 0x3f;

        for (j = 0; (j < i + 1); j++)
            ret += base64_chars[char_array_4[j]];

        while((i++ < 3))
            ret += '=';
    }

    return ret;
}

std::vector<uint8_t> base64_decode(const std::string& encoded_string) {
    int len = encoded_string.size();
    int i = 0;
    int j = 0;
    int in = 0;
    unsigned char char_array_4[4], char_array_3[3];
    std::vector<uint8_t> ret;

    while (len-- && ( encoded_string[in] != '=') && is_base64(encoded_string[in])) {
        char_array_4[i++] = encoded_string[in]; in++;
        if (i ==4) {
            for (i = 0; i <4; i++)
                char_array_4[i] = std::find(base64_chars, base64_chars + 64, char_array_4[i]) - base64_chars;

            char_array_3[0] = (char_array_4[0] << 2) + ((char_array_4[1] & 0x30) >> 4);
            char_array_3[1] = ((char_array_4[1] & 0xf) << 4) + ((char_array_4[2] & 0x3c) >> 2);
            char_array_3[2] = ((char_array_4[2] & 0x3) << 6) + char_array_4[3];

            for (i = 0; (i < 3); i++)
                ret.push_back(char_array_3[i]);
            i = 0;
        }
    }

    if (i) {
        for (j = i; j <4; j++)
            char_array_4[j] = 0;

        for (j = 0; j <4; j++)
            char_array_4[j] = std::find(base64_chars, base64_chars + 64, char_array_4[j]) - base64_chars;

        char_array_3[0] = (char_array_4[0] << 2) + ((char_array_4[1] & 0x30) >> 4);
        char_array_3[1] = ((char_array_4[1] & 0xf) << 4) + ((char_array_4[2] & 0x3c) >> 2);
        char_array_3[2] = ((char_array_4[2] & 0x3) << 6) + char_array_4[3];

        for (j = 0; (j < i - 1); j++) ret.push_back(char_array_3[j]);
    }

    return ret;
}./common/websocket_client.hpp
#pragma once
#include <boost/beast/core.hpp>
#include <boost/beast/websocket.hpp>
#include <boost/beast/websocket/ssl.hpp>
#include <boost/beast/ssl.hpp>
#include <boost/asio/ssl.hpp>
#include <functional>
#include <string>
#include <thread>
#include <atomic>

namespace beast = boost::beast;
namespace websocket = beast::websocket;
namespace net = boost::asio;
namespace ssl = boost::asio::ssl;
using tcp = boost::asio::ip::tcp;

class WebSocketClient {
private:
    net::io_context ioc;
    ssl::context ctx{ssl::context::tlsv12_client};
    tcp::resolver resolver{ioc};
    websocket::stream<beast::ssl_stream<tcp::socket>> ws{ioc, ctx};

    std::thread ioThread;
    std::atomic<bool> shouldStop{false};

    std::function<void()> onConnected;
    std::function<void()> onDisconnected;
    std::function<void(const std::string&)> onMessage;

public:
    WebSocketClient();
    ~WebSocketClient();

    bool connect(const std::string& host, const std::string& port, const std::string& target);
    void disconnect();
    void send(const std::string& message);

    void setOnConnected(std::function<void()> handler) { onConnected = handler; }
    void setOnDisconnected(std::function<void()> handler) { onDisconnected = handler; }
    void setOnMessage(std::function<void(const std::string&)> handler) { onMessage = handler; }

private:
    void runEventLoop();
};
./common/base64.hpp
#pragma once
#include <string>
#include <vector>
#include <cstdint>

std::string base64_encode(const std::vector<uint8_t>& data);
std::vector<uint8_t> base64_decode(const std::string& encoded);./Dockefile
# docker/client/Dockerfile
# Multi-stage build: build with vcpkg + CMake, then ship a slim runtime
ARG UBUNTU_VERSION=22.04
FROM ubuntu:${UBUNTU_VERSION} AS build

ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential cmake git curl unzip zip tar pkg-config ninja-build ca-certificates \
    python3 \
    libvulkan1 mesa-vulkan-drivers \
    ocl-icd-opencl-dev \
    && rm -rf /var/lib/apt/lists/*

# Install vcpkg
ENV VCPKG_ROOT=/opt/vcpkg
RUN git clone --depth=1 https://github.com/microsoft/vcpkg.git ${VCPKG_ROOT} \
 && ${VCPKG_ROOT}/bootstrap-vcpkg.sh -disableMetrics

ARG VCPKG_TRIPLET=x64-linux

# Install only what we need — rely on system libvulkan loader
RUN ${VCPKG_ROOT}/vcpkg install --triplet=${VCPKG_TRIPLET} \
    boost-system \
    boost-url \
    boost-beast \
    openssl \
    nlohmann-json \
    shaderc \
    vulkan-headers

# Build client
WORKDIR /src
COPY . /src

ARG BUILD_TYPE=Release
RUN cmake -S . -B build \
    -G Ninja \
    -DCMAKE_BUILD_TYPE=${BUILD_TYPE} \
    -DCMAKE_TOOLCHAIN_FILE=${VCPKG_ROOT}/scripts/buildsystems/vcpkg.cmake \
    -DVCPKG_TARGET_TRIPLET=${VCPKG_TRIPLET} \
    -DVulkan_INCLUDE_DIR=${VCPKG_ROOT}/installed/${VCPKG_TRIPLET}/include \
    -DVulkan_LIBRARY=/lib/x86_64-linux-gnu/libvulkan.so.1 \
    -DOPENCL_INCLUDE_DIR=/usr/include \
    -DOPENCL_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/libOpenCL.so \
    -DENABLE_CUDA=OFF \
 && cmake --build build --parallel \
 && cmake --install build --prefix /opt/mfc

# Runtime stage
FROM ubuntu:${UBUNTU_VERSION} AS runtime
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    libstdc++6 \
    libvulkan1 mesa-vulkan-drivers \
    ocl-icd-libopencl1 \
    && rm -rf /var/lib/apt/lists/*

COPY --from=build /opt/mfc /opt/mfc
COPY --from=build /opt/vcpkg/installed /opt/vcpkg/installed
ENV LD_LIBRARY_PATH=/opt/vcpkg/installed/${VCPKG_TRIPLET}/lib:/opt/vcpkg/installed/${VCPKG_TRIPLET}/lib/manual-link:${LD_LIBRARY_PATH}

WORKDIR /opt/mfc/bin

ENV FRAMEWORK=vulkan
ENV SERVER_URL=https://localhost:3000
ENTRYPOINT ["/bin/sh", "-lc", "exec ./${FRAMEWORK}_client --server ${SERVER_URL}"]
./CMakeLists.txt
cmake_minimum_required(VERSION 3.20)
project(MultiFrameworkClient LANGUAGES CXX)

# Set C++ standard
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# Enable vcpkg toolchain integration
if(DEFINED ENV{VCPKG_ROOT} AND NOT DEFINED CMAKE_TOOLCHAIN_FILE)
    set(CMAKE_TOOLCHAIN_FILE "$ENV{VCPKG_ROOT}/scripts/buildsystems/vcpkg.cmake"
        CACHE STRING "")
endif()

option(ENABLE_CUDA "Enable CUDA support" OFF)
if (ENABLE_CUDA)
  enable_language(CUDA)
endif()

# Find required packages via vcpkg
find_package(Boost REQUIRED COMPONENTS system url)
find_package(OpenSSL REQUIRED)
find_package(nlohmann_json CONFIG REQUIRED)
find_package(Threads REQUIRED)

# Optional: CUDA support
option(ENABLE_CUDA "Enable CUDA support" ON)
if(ENABLE_CUDA)
    enable_language(CUDA)
    find_package(CUDAToolkit QUIET)
    if(CUDAToolkit_FOUND)
        message(STATUS "CUDA found - enabling CUDA executor")
        set(HAVE_CUDA ON)
    else()
        message(WARNING "CUDA not found - disabling CUDA executor")
        set(HAVE_CUDA OFF)
    endif()
endif()

# Fix CUDA::cudart target if missing
if(HAVE_CUDA AND NOT TARGET CUDA::cudart)
    find_library(CUDART_LIBRARY NAMES cudart libcudart PATHS
        /usr/local/cuda/lib64
        /usr/local/cuda-*/targets/x86_64-linux/lib
        /usr/local/cuda-*/lib64
        NO_DEFAULT_PATH
    )

    find_path(CUDART_INCLUDE_DIR cuda_runtime.h PATHS
        /usr/local/cuda/include
        /usr/local/cuda-*/targets/x86_64-linux/include
        NO_DEFAULT_PATH
    )

    if(CUDART_LIBRARY)
        message(STATUS "Creating imported target CUDA::cudart -> ${CUDART_LIBRARY}")
        add_library(CUDA::cudart UNKNOWN IMPORTED)
        set_target_properties(CUDA::cudart PROPERTIES
            IMPORTED_LOCATION "${CUDART_LIBRARY}"
            INTERFACE_INCLUDE_DIRECTORIES "${CUDART_INCLUDE_DIR}"
        )
    else()
        message(WARNING "Could not find libcudart; CUDA runtime functions may link-fail")
        set(HAVE_CUDA OFF)
    endif()
endif()

# Fix CUDA::nvrtc target if missing
if(HAVE_CUDA AND NOT TARGET CUDA::nvrtc)
    message(STATUS "CUDA::nvrtc target not found attempting manual nvrtc detection")

    set(_cuda_hint_paths
        $ENV{CUDA_HOME}
        /usr/local/cuda-13
        /usr/local/cuda
        /usr
        /usr/local
    )

    find_path(NVRTC_INCLUDE_DIR nvrtc.h
        HINTS ${_cuda_hint_paths}
        PATH_SUFFIXES include include/cuda include/x86_64-linux-gnu/include
    )

    find_library(NVRTC_LIBRARY NAMES nvrtc nvrtc.so nvrtc.so.13 nvrtc.so.12
        HINTS ${_cuda_hint_paths}
        PATH_SUFFIXES lib lib64 targets/x86_64-linux/lib lib/x86_64-linux-gnu
    )

    if(NVRTC_LIBRARY AND NVRTC_INCLUDE_DIR)
        message(STATUS "Found nvrtc: ${NVRTC_LIBRARY}; include: ${NVRTC_INCLUDE_DIR}")
        add_library(CUDA::nvrtc SHARED IMPORTED)
        set_target_properties(CUDA::nvrtc PROPERTIES
            IMPORTED_LOCATION "${NVRTC_LIBRARY}"
            INTERFACE_INCLUDE_DIRECTORIES "${NVRTC_INCLUDE_DIR}"
        )
    else()
        message(WARNING "nvrtc library or header not found; CUDA NVRTC functionality will be unavailable.")
        set(HAVE_CUDA OFF)
    endif()
endif()

# Optional: OpenCL support - WORKING VERSION
option(ENABLE_OPENCL "Enable OpenCL support" ON)
if(ENABLE_OPENCL)
    # Look for OpenCL headers in common locations
    find_path(OPENCL_INCLUDE_DIR CL/cl.h
        PATHS
        /usr/local/cuda-11.8/targets/x86_64-linux/include
        /usr/local/cuda/include
        /usr/include
        /usr/local/include
        /opt/AMDAPPSDK-3.0/include
    )

    # Find the OpenCL library that -lOpenCL would link to
    find_library(OPENCL_LIBRARY_PATH NAMES OpenCL
        PATHS
        /usr/lib/x86_64-linux-gnu
        /usr/lib
        /usr/local/lib
        /lib/x86_64-linux-gnu
        /opt/AMDAPPSDK-3.0/lib/x86_64
    )

    message(STATUS "OpenCL detection:")
    message(STATUS "  Headers: ${OPENCL_INCLUDE_DIR}")
    message(STATUS "  Library: ${OPENCL_LIBRARY_PATH}")

    if(OPENCL_INCLUDE_DIR AND OPENCL_LIBRARY_PATH)
        message(STATUS "OpenCL found - enabling OpenCL executor")
        set(HAVE_OPENCL ON)

        # Create a simple OpenCL target that mimics successful compilation
        add_library(SimpleOpenCL INTERFACE)
        target_include_directories(SimpleOpenCL INTERFACE ${OPENCL_INCLUDE_DIR})
        target_link_libraries(SimpleOpenCL INTERFACE ${OPENCL_LIBRARY_PATH})

    else()
        message(WARNING "OpenCL not found - disabling OpenCL executor")
        message(WARNING "  Looked for CL/cl.h in standard locations")
        message(WARNING "  Looked for libOpenCL in standard locations")
        set(HAVE_OPENCL OFF)
    endif()
else()
    message(STATUS "OpenCL support disabled by user")
    set(HAVE_OPENCL OFF)
endif()

# Optional: Vulkan support
option(ENABLE_VULKAN "Enable Vulkan support" ON)
if(ENABLE_VULKAN)
    find_package(Vulkan QUIET)
    if(Vulkan_FOUND)
        # Try to find shaderc for runtime shader compilation
        find_package(unofficial-shaderc CONFIG QUIET)
        if(unofficial-shaderc_FOUND)
            message(STATUS "Vulkan and shaderc found - enabling Vulkan executor")
            set(HAVE_VULKAN ON)
            set(HAVE_SHADERC ON)
        else()
            # Try to find shaderc manually
            find_path(SHADERC_INCLUDE_DIR shaderc/shaderc.hpp
                PATHS
                /usr/include
                /usr/local/include
                /opt/vulkan-sdk/include
                $ENV{VULKAN_SDK}/include
            )

            find_library(SHADERC_LIBRARY NAMES shaderc_shared shaderc
                PATHS
                /usr/lib
                /usr/local/lib
                /opt/vulkan-sdk/lib
                $ENV{VULKAN_SDK}/lib
            )

            if(SHADERC_INCLUDE_DIR AND SHADERC_LIBRARY)
                message(STATUS "Vulkan and shaderc found manually - enabling Vulkan executor")
                set(HAVE_VULKAN ON)
                set(HAVE_SHADERC ON)

                # Create shaderc target
                add_library(shaderc_shared SHARED IMPORTED)
                set_target_properties(shaderc_shared PROPERTIES
                    IMPORTED_LOCATION "${SHADERC_LIBRARY}"
                    INTERFACE_INCLUDE_DIRECTORIES "${SHADERC_INCLUDE_DIR}"
                )
            else()
                message(WARNING "Vulkan found but shaderc not found - disabling Vulkan executor")
                message(WARNING "  Install shaderc development libraries for Vulkan support")
                set(HAVE_VULKAN OFF)
                set(HAVE_SHADERC OFF)
            endif()
        endif()
    else()
        message(WARNING "Vulkan not found - disabling Vulkan executor")
        set(HAVE_VULKAN OFF)
        set(HAVE_SHADERC OFF)
    endif()
endif()

# Include directories
include_directories(${CMAKE_CURRENT_SOURCE_DIR})

# Add CUDA include directories if available
if(HAVE_CUDA)
    foreach(_p
        $ENV{CUDA_HOME}
        /usr/local/cuda-13/targets/x86_64-linux/include
        /usr/local/cuda-13/include
        /usr/local/cuda/include
    )
        if(_p AND EXISTS "${_p}")
            message(STATUS "Adding CUDA include dir: ${_p}")
            include_directories(BEFORE "${_p}")
            break()
        endif()
    endforeach()
endif()

# Common source files
set(COMMON_SOURCES
    common/framework_client.cpp
    common/websocket_client.cpp
    common/base64.cpp
)

# Base libraries that all targets need
set(BASE_LIBS
    Boost::system
    Boost::url
    OpenSSL::SSL
    OpenSSL::Crypto
    nlohmann_json::nlohmann_json
    Threads::Threads
)

# CUDA Client
if(HAVE_CUDA)
    add_executable(cuda_client
        main.cpp
        ${COMMON_SOURCES}
        cuda/cuda_executor.cpp
    )

    target_link_libraries(cuda_client PRIVATE
        ${BASE_LIBS}
        CUDA::cuda_driver
        CUDA::nvrtc
        CUDA::cudart
    )

    target_compile_definitions(cuda_client PRIVATE HAVE_CUDA=1  CLIENT_CUDA=1)

    set_target_properties(cuda_client PROPERTIES
        RUNTIME_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/bin"
    )

    message(STATUS "Created target: cuda_client")
endif()

# OpenCL Client - WORKING VERSION
if(HAVE_OPENCL)
    add_executable(opencl_client
        main.cpp
        ${COMMON_SOURCES}
        opencl/opencl_executor.cpp
    )

    target_link_libraries(opencl_client PRIVATE
        ${BASE_LIBS}
        SimpleOpenCL  # Use our working OpenCL target
    )

    target_compile_definitions(opencl_client PRIVATE HAVE_OPENCL=1  CLIENT_OPENCL=1)

    set_target_properties(opencl_client PROPERTIES
        RUNTIME_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/bin"
    )

    message(STATUS "Created target: opencl_client")
endif()

# Vulkan Client
if(HAVE_VULKAN)
    add_executable(vulkan_client
        main.cpp
        ${COMMON_SOURCES}
        vulkan/vulkan_executor.cpp
    )

    set(VULKAN_LIBS ${BASE_LIBS} Vulkan::Vulkan)

    # Add shaderc library
    if(unofficial-shaderc_FOUND)
        list(APPEND VULKAN_LIBS unofficial::shaderc::shaderc)
    else()
        list(APPEND VULKAN_LIBS shaderc_shared)
    endif()

    target_link_libraries(vulkan_client PRIVATE ${VULKAN_LIBS})

    target_compile_definitions(vulkan_client PRIVATE HAVE_VULKAN=1  CLIENT_VULKAN=1)

    set_target_properties(vulkan_client PROPERTIES
        RUNTIME_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/bin"
    )

    message(STATUS "Created target: vulkan_client")
endif()

# Universal client that can be built with all available frameworks
if(HAVE_CUDA OR HAVE_OPENCL OR HAVE_VULKAN)
    set(UNIVERSAL_SOURCES ${COMMON_SOURCES})
    set(UNIVERSAL_LIBS ${BASE_LIBS})
    set(UNIVERSAL_DEFS "")

    if(HAVE_CUDA)
        list(APPEND UNIVERSAL_SOURCES cuda/cuda_executor.cpp)
        list(APPEND UNIVERSAL_LIBS CUDA::cuda_driver CUDA::nvrtc CUDA::cudart)
        list(APPEND UNIVERSAL_DEFS HAVE_CUDA=1)
    endif()

    if(HAVE_OPENCL)
        list(APPEND UNIVERSAL_SOURCES opencl/opencl_executor.cpp)
        list(APPEND UNIVERSAL_LIBS SimpleOpenCL)
        list(APPEND UNIVERSAL_DEFS HAVE_OPENCL=1)
    endif()

    if(HAVE_VULKAN)
        list(APPEND UNIVERSAL_SOURCES vulkan/vulkan_executor.cpp)
        list(APPEND UNIVERSAL_LIBS Vulkan::Vulkan)
        list(APPEND UNIVERSAL_DEFS HAVE_VULKAN=1)

        # Add shaderc to universal client
        if(unofficial-shaderc_FOUND)
            list(APPEND UNIVERSAL_LIBS unofficial::shaderc::shaderc)
        else()
            list(APPEND UNIVERSAL_LIBS shaderc_shared)
        endif()
    endif()

    add_executable(universal_client
        main.cpp
        ${UNIVERSAL_SOURCES}
    )

    target_link_libraries(universal_client PRIVATE ${UNIVERSAL_LIBS})
    target_compile_definitions(universal_client PRIVATE ${UNIVERSAL_DEFS}   CLIENT_UNIVERSAL=1)

    set_target_properties(universal_client PROPERTIES
        RUNTIME_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/bin"
    )

    message(STATUS "Created target: universal_client")
else()
    message(WARNING "No compute frameworks found - no executables will be built")
endif()

# Print build summary
message(STATUS "=== Build Summary ===")
message(STATUS "CUDA Support: ${HAVE_CUDA}")
message(STATUS "OpenCL Support: ${HAVE_OPENCL}")
message(STATUS "Vulkan Support: ${HAVE_VULKAN}")
if(HAVE_VULKAN)
    message(STATUS "Shaderc Support: ${HAVE_SHADERC}")
endif()
if(HAVE_OPENCL)
    message(STATUS "OpenCL Include: ${OPENCL_INCLUDE_DIR}")
    message(STATUS "OpenCL Library: ${OPENCL_LIBRARY_PATH}")
endif()
message(STATUS "====================")

# Installation rules
set(INSTALL_TARGETS "")

if(HAVE_CUDA OR HAVE_OPENCL OR HAVE_VULKAN)
    list(APPEND INSTALL_TARGETS universal_client)
endif()

if(HAVE_CUDA)
    list(APPEND INSTALL_TARGETS cuda_client)
endif()

if(HAVE_OPENCL)
    list(APPEND INSTALL_TARGETS opencl_client)
endif()

if(HAVE_VULKAN)
    list(APPEND INSTALL_TARGETS vulkan_client)
endif()

if(INSTALL_TARGETS)
    install(TARGETS ${INSTALL_TARGETS} DESTINATION bin)
    message(STATUS "Will install: ${INSTALL_TARGETS}")
else()
    message(STATUS "No targets to install")
endif()./opencl/opencl_executor.cpp
// opencl/opencl_executor.cpp
#include "opencl_executor.hpp"
#include <iostream>
#include <fstream>
#include <chrono>

OpenCLExecutor::OpenCLExecutor() {}

OpenCLExecutor::~OpenCLExecutor() {
    cleanup();
}

bool OpenCLExecutor::initialize(const json& config) {
    if (initialized) return true;

    cl_int err;

    // Get platform
    cl_uint numPlatforms;
    err = clGetPlatformIDs(0, nullptr, &numPlatforms);
    if (err != CL_SUCCESS || numPlatforms == 0) {
        std::cerr << "No OpenCL platforms found" << std::endl;
        return false;
    }

    std::vector<cl_platform_id> platforms(numPlatforms);
    err = clGetPlatformIDs(numPlatforms, platforms.data(), nullptr);
    if (err != CL_SUCCESS) {
        std::cerr << "Failed to get OpenCL platforms" << std::endl;
        return false;
    }

    // Use first platform or user-specified
    int platformId = config.value("platformId", 0);
    if (platformId >= static_cast<int>(numPlatforms)) {
        platformId = 0;
    }
    platform = platforms[platformId];

    // Get platform info
    char platformName[256];
    clGetPlatformInfo(platform, CL_PLATFORM_NAME, sizeof(platformName), platformName, nullptr);
    std::cout << "Using OpenCL platform: " << platformName << std::endl;

    // Get devices
    cl_uint numDevices;
    cl_device_type deviceType = CL_DEVICE_TYPE_GPU;

    if (config.contains("deviceType")) {
        std::string devTypeStr = config["deviceType"];
        if (devTypeStr == "CPU") deviceType = CL_DEVICE_TYPE_CPU;
        else if (devTypeStr == "GPU") deviceType = CL_DEVICE_TYPE_GPU;
        else if (devTypeStr == "ALL") deviceType = CL_DEVICE_TYPE_ALL;
    }

    err = clGetDeviceIDs(platform, deviceType, 0, nullptr, &numDevices);
    if (err != CL_SUCCESS || numDevices == 0) {
        std::cerr << "No suitable OpenCL devices found" << std::endl;
        return false;
    }

    std::vector<cl_device_id> devices(numDevices);
    err = clGetDeviceIDs(platform, deviceType, numDevices, devices.data(), nullptr);
    if (err != CL_SUCCESS) {
        std::cerr << "Failed to get OpenCL devices" << std::endl;
        return false;
    }

    // Use first device or user-specified
    int deviceId = config.value("deviceId", 0);
    if (deviceId >= static_cast<int>(numDevices)) {
        deviceId = 0;
    }
    device = devices[deviceId];

    // Get device info
    char deviceName[256];
    clGetDeviceInfo(device, CL_DEVICE_NAME, sizeof(deviceName), deviceName, nullptr);
    std::cout << "Using OpenCL device: " << deviceName << std::endl;

    // Create context
    context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &err);
    if (err != CL_SUCCESS) {
        std::cerr << "Failed to create OpenCL context" << std::endl;
        return false;
    }

    // Create command queue
    queue = clCreateCommandQueue(context, device, 0, &err);
    if (err != CL_SUCCESS) {
        std::cerr << "Failed to create OpenCL command queue" << std::endl;
        return false;
    }

    initialized = true;
    return true;
}

void OpenCLExecutor::cleanup() {
    if (!initialized) return;

    // Cleanup cached kernels
    for (auto& [key, kernel] : kernelCache) {
        if (kernel.kernel) clReleaseKernel(kernel.kernel);
        if (kernel.program) clReleaseProgram(kernel.program);
    }
    kernelCache.clear();

    if (queue) {
        clReleaseCommandQueue(queue);
        queue = nullptr;
    }

    if (context) {
        clReleaseContext(context);
        context = nullptr;
    }

    initialized = false;
}

bool OpenCLExecutor::compileKernel(const std::string& source, const std::string& entryPoint,
                                  const json& compileOpts, CompiledKernel& result) {
    cl_int err;

    // Create program from source
    const char* sourcePtr = source.c_str();
    size_t sourceSize = source.length();

    result.program = clCreateProgramWithSource(context, 1, &sourcePtr, &sourceSize, &err);
    if (err != CL_SUCCESS) {
        std::cerr << "Failed to create OpenCL program" << std::endl;
        return false;
    }

    // Build program
    std::string buildOptions;
    if (compileOpts.contains("buildOptions")) {
        buildOptions = compileOpts["buildOptions"];
    }

    err = clBuildProgram(result.program, 1, &device, buildOptions.c_str(), nullptr, nullptr);
    if (err != CL_SUCCESS) {
        // Get build log
        size_t logSize;
        clGetProgramBuildInfo(result.program, device, CL_PROGRAM_BUILD_LOG, 0, nullptr, &logSize);

        std::vector<char> buildLog(logSize);
        clGetProgramBuildInfo(result.program, device, CL_PROGRAM_BUILD_LOG, logSize, buildLog.data(), nullptr);

        std::cerr << "OpenCL build failed:\n" << buildLog.data() << std::endl;

        clReleaseProgram(result.program);
        return false;
    }

    // Create kernel
    result.kernel = clCreateKernel(result.program, entryPoint.c_str(), &err);
    if (err != CL_SUCCESS) {
        std::cerr << "Failed to create OpenCL kernel: " << entryPoint << std::endl;
        clReleaseProgram(result.program);
        return false;
    }

    return true;
}

bool OpenCLExecutor::createInputBuffers(const TaskData& task, BufferSet& buffers) {
    cl_int err;

    for (size_t i = 0; i < task.inputData.size(); i++) {
        const auto& inputData = task.inputData[i];

        if (inputData.empty()) {
            // Empty input - create a small placeholder buffer
            cl_mem buffer = clCreateBuffer(context, CL_MEM_READ_ONLY, 4, nullptr, &err);
            if (err != CL_SUCCESS) {
                std::cerr << "Failed to create placeholder input buffer " << i << std::endl;
                return false;
            }
            buffers.inputBuffers.push_back(buffer);
        } else {
            // Create buffer with data
            cl_mem buffer = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,
                                          inputData.size(), const_cast<uint8_t*>(inputData.data()), &err);
            if (err != CL_SUCCESS) {
                std::cerr << "Failed to create input buffer " << i << std::endl;
                return false;
            }
            buffers.inputBuffers.push_back(buffer);
        }
    }

    // If no inputs provided, create one from legacy data for backward compatibility
    if (buffers.inputBuffers.empty() && !task.legacyInputData.empty()) {
        cl_mem buffer = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,
                                      task.legacyInputData.size(),
                                      const_cast<uint8_t*>(task.legacyInputData.data()), &err);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to create legacy input buffer" << std::endl;
            return false;
        }
        buffers.inputBuffers.push_back(buffer);
    }

    return true;
}

bool OpenCLExecutor::createOutputBuffers(const TaskData& task, BufferSet& buffers) {
    cl_int err;

    for (size_t i = 0; i < task.outputSizes.size(); i++) {
        size_t outputSize = task.outputSizes[i];

        cl_mem buffer = clCreateBuffer(context, CL_MEM_WRITE_ONLY, outputSize, nullptr, &err);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to create output buffer " << i << " (size: " << outputSize << ")" << std::endl;
            return false;
        }
        buffers.outputBuffers.push_back(buffer);
    }

    // If no output sizes provided, create one from legacy size for backward compatibility
    if (buffers.outputBuffers.empty() && task.legacyOutputSize > 0) {
        cl_mem buffer = clCreateBuffer(context, CL_MEM_WRITE_ONLY, task.legacyOutputSize, nullptr, &err);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to create legacy output buffer" << std::endl;
            return false;
        }
        buffers.outputBuffers.push_back(buffer);
    }

    return true;
}

bool OpenCLExecutor::setKernelArguments(cl_kernel kernel, const BufferSet& buffers, const TaskData& task) {
    cl_int err;
    int argIndex = 0;

    // Set input buffers as arguments
    for (size_t i = 0; i < buffers.inputBuffers.size(); i++) {
        err = clSetKernelArg(kernel, argIndex++, sizeof(cl_mem), &buffers.inputBuffers[i]);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to set input buffer argument " << i << " at index " << (argIndex-1) << std::endl;
            return false;
        }
    }

    // Set output buffers as arguments
    for (size_t i = 0; i < buffers.outputBuffers.size(); i++) {
        err = clSetKernelArg(kernel, argIndex++, sizeof(cl_mem), &buffers.outputBuffers[i]);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to set output buffer argument " << i << " at index " << (argIndex-1) << std::endl;
            return false;
        }
    }

    // Set additional arguments from chunk uniforms if present
    if (!task.chunkUniforms.empty()) {
        for (auto& [key, value] : task.chunkUniforms.items()) {
            if (value.is_number_integer()) {
                int intVal = value;
                err = clSetKernelArg(kernel, argIndex++, sizeof(int), &intVal);
            } else if (value.is_number_float()) {
                float floatVal = value;
                err = clSetKernelArg(kernel, argIndex++, sizeof(float), &floatVal);
            } else if (value.is_number_unsigned()) {
                unsigned int uintVal = value;
                err = clSetKernelArg(kernel, argIndex++, sizeof(unsigned int), &uintVal);
            } else {
                std::cerr << "Unsupported uniform type for key: " << key << std::endl;
                continue;
            }

            if (err != CL_SUCCESS) {
                std::cerr << "Failed to set chunk uniform argument: " << key << " at index " << (argIndex-1) << std::endl;
                return false;
            }
        }
    }

    return true;
}

bool OpenCLExecutor::readOutputBuffers(const BufferSet& buffers, const TaskData& task, TaskResult& result) {
    cl_int err;

    // Read all output buffers
    for (size_t i = 0; i < buffers.outputBuffers.size(); i++) {
        size_t outputSize = (i < task.outputSizes.size()) ? task.outputSizes[i] : task.legacyOutputSize;

        std::vector<uint8_t> outputData(outputSize);
        err = clEnqueueReadBuffer(queue, buffers.outputBuffers[i], CL_TRUE, 0, outputSize,
                                 outputData.data(), 0, nullptr, nullptr);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to read output buffer " << i << std::endl;
            return false;
        }

        result.outputData.push_back(std::move(outputData));
    }

    // Set legacy output for backward compatibility
    if (!result.outputData.empty()) {
        result.legacyOutputData = result.outputData[0];
    }

    return true;
}

TaskResult OpenCLExecutor::executeTask(const TaskData& task) {
    TaskResult result;
    auto startTime = std::chrono::high_resolution_clock::now();

    if (!initialized) {
        result.success = false;
        result.errorMessage = "OpenCL not initialized";
        return result;
    }

    BufferSet buffers;

    try {
        // Compile kernel if not cached
        std::string cacheKey = task.kernel + "|" + task.entry;
        auto it = kernelCache.find(cacheKey);
        CompiledKernel* kernelPtr;

        if (it == kernelCache.end()) {
            CompiledKernel newKernel;
            if (!compileKernel(task.kernel, task.entry, task.compilationOptions, newKernel)) {
                result.success = false;
                result.errorMessage = "Kernel compilation failed";
                return result;
            }
            kernelCache[cacheKey] = std::move(newKernel);
            kernelPtr = &kernelCache[cacheKey];
        } else {
            kernelPtr = &it->second;
        }

        std::cout << "Executing OpenCL kernel with " << task.getInputCount()
                  << " inputs and " << task.getOutputCount() << " outputs" << std::endl;

        // Create input buffers
        if (!createInputBuffers(task, buffers)) {
            result.success = false;
            result.errorMessage = "Failed to create input buffers";
            return result;
        }

        // Create output buffers
        if (!createOutputBuffers(task, buffers)) {
            result.success = false;
            result.errorMessage = "Failed to create output buffers";
            return result;
        }

        // Set kernel arguments
        if (!setKernelArguments(kernelPtr->kernel, buffers, task)) {
            result.success = false;
            result.errorMessage = "Failed to set kernel arguments";
            return result;
        }

        // Execute kernel
        size_t globalWorkSize[3] = {
            static_cast<size_t>(task.workgroupCount.size() > 0 ? task.workgroupCount[0] : 1),
            static_cast<size_t>(task.workgroupCount.size() > 1 ? task.workgroupCount[1] : 1),
            static_cast<size_t>(task.workgroupCount.size() > 2 ? task.workgroupCount[2] : 1)
        };

        cl_int err = clEnqueueNDRangeKernel(queue, kernelPtr->kernel, 3, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);
        if (err != CL_SUCCESS) {
            result.success = false;
            result.errorMessage = "Kernel execution failed with error: " + std::to_string(err);
            return result;
        }

        // Wait for completion
        err = clFinish(queue);
        if (err != CL_SUCCESS) {
            result.success = false;
            result.errorMessage = "Failed to wait for kernel completion";
            return result;
        }

        // Read results from all output buffers
        if (!readOutputBuffers(buffers, task, result)) {
            result.success = false;
            result.errorMessage = "Failed to read output buffers";
            return result;
        }

        auto endTime = std::chrono::high_resolution_clock::now();
        result.processingTime = std::chrono::duration<double, std::milli>(endTime - startTime).count();
        result.success = true;

        std::cout << "OpenCL execution completed: " << result.getOutputCount() << " outputs, "
                  << result.processingTime << "ms" << std::endl;

    } catch (const std::exception& e) {
        result.success = false;
        result.errorMessage = std::string("Exception: ") + e.what();
    }

    return result;
}

json OpenCLExecutor::getCapabilities() const {
    json caps;
    caps["framework"] = "opencl";
    caps["initialized"] = initialized;
    caps["supportsMultiInput"] = true;  // NEW: Advertise multi-input/output support
    caps["supportsMultiOutput"] = true;
    caps["maxInputs"] = 4;              // NEW: Practical limits
    caps["maxOutputs"] = 3;

    if (initialized && device) {
        char deviceName[256];
        char deviceVendor[256];
        cl_device_type deviceType;
        cl_ulong globalMemSize;
        cl_uint maxComputeUnits;

        clGetDeviceInfo(device, CL_DEVICE_NAME, sizeof(deviceName), deviceName, nullptr);
        clGetDeviceInfo(device, CL_DEVICE_VENDOR, sizeof(deviceVendor), deviceVendor, nullptr);
        clGetDeviceInfo(device, CL_DEVICE_TYPE, sizeof(deviceType), &deviceType, nullptr);
        clGetDeviceInfo(device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof(globalMemSize), &globalMemSize, nullptr);
        clGetDeviceInfo(device, CL_DEVICE_MAX_COMPUTE_UNITS, sizeof(maxComputeUnits), &maxComputeUnits, nullptr);

        caps["device"] = {
            {"name", deviceName},
            {"vendor", deviceVendor},
            {"type", (deviceType == CL_DEVICE_TYPE_GPU) ? "GPU" :
                    (deviceType == CL_DEVICE_TYPE_CPU) ? "CPU" : "Other"},
            {"globalMemory", globalMemSize},
            {"computeUnits", maxComputeUnits}
        };
    }

    return caps;
}./opencl/opencl_executor.hpp
#pragma once
#include "../common/framework_client.hpp"
#ifdef HAVE_OPENCL
#ifdef __APPLE__
#include <OpenCL/opencl.h>
#else
#include <CL/cl.h>
#endif
#include <vector>
#include <memory>

class OpenCLExecutor : public IFrameworkExecutor {
private:
    cl_platform_id platform = nullptr;
    cl_device_id device = nullptr;
    cl_context context = nullptr;
    cl_command_queue queue = nullptr;
    bool initialized = false;

    struct CompiledKernel {
        cl_program program = nullptr;
        cl_kernel kernel = nullptr;
    };

    std::map<std::string, CompiledKernel> kernelCache;

    bool compileKernel(const std::string& source, const std::string& entryPoint,
                      const json& compileOpts, CompiledKernel& result);

    struct BufferSet {
        std::vector<cl_mem> inputBuffers;
        std::vector<cl_mem> outputBuffers;

        void clear() {
            for (auto buf : inputBuffers) {
                if (buf) clReleaseMemObject(buf);
            }
            for (auto buf : outputBuffers) {
                if (buf) clReleaseMemObject(buf);
            }
            inputBuffers.clear();
            outputBuffers.clear();
        }

        ~BufferSet() {
            clear();
        }
    };

    bool createInputBuffers(const TaskData& task, BufferSet& buffers);
    bool createOutputBuffers(const TaskData& task, BufferSet& buffers);
    bool setKernelArguments(cl_kernel kernel, const BufferSet& buffers, const TaskData& task);
    bool readOutputBuffers(const BufferSet& buffers, const TaskData& task, TaskResult& result);

public:
    OpenCLExecutor();
    ~OpenCLExecutor() override;

    bool initialize(const json& config = {}) override;
    void cleanup() override;
    TaskResult executeTask(const TaskData& task) override;
    std::string getFrameworkName() const override { return "opencl"; }
    json getCapabilities() const override;
};
#endif./whole_code.txt
./kernels_smoke/vulkan_smoke.comp
#version 450
layout(local_size_x = 64) in;
layout(set=0, binding=1) buffer Out { uint outData[]; };
void main() {
  uint i = gl_GlobalInvocationID.x;
  outData[i] = i;
}
./kernels_smoke/webgl_smoke.glsl
#ifdef GL_ES
precision highp float;
#endif
out vec4 fragColor;
void main() {
  fragColor = vec4(0.1, 0.6, 0.2, 1.0);
}
./kernels_smoke/opencl_smoke.cl
__kernel void kernel(__global uint* out) {
  uint i = get_global_id(0);
  out[i] = i;
}
./kernels_smoke/webgpu_smoke.wgsl
@group(0) @binding(1) var<storage, read_write> outData : array<u32>;
@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) gid : vec3<u32>) {
  let i = gid.x;
  if (i < arrayLength(&outData)) {
    outData[i] = i;
  }
}
./kernels_smoke/cuda_smoke.cu
extern "C" __global__ void kernel(unsigned int* out) {
  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
  out[i] = i;
}
./vulkan/vulkan_executor.cpp
#include "vulkan_executor.hpp"
#include <iostream>
#include <vector>
#include <cstring>
#include <chrono>
#include <stdexcept>
#include <algorithm>

// Shaderc
#include <shaderc/shaderc.hpp>

// ========== Utility: scoped VkResult check ==========
#define VK_CHECK(x) do { VkResult _ret = (x); if (_ret != VK_SUCCESS) { \
    throw std::runtime_error(std::string("Vulkan error: ") + #x + " -> " + std::to_string(_ret)); } } while(0)

VulkanExecutor::VulkanExecutor() {}
VulkanExecutor::~VulkanExecutor() { cleanup(); }

// -------- Instance & device --------
bool VulkanExecutor::createInstance(bool enableValidation, bool enableDebugUtils) {
    // Query available instance extensions
    uint32_t extCount = 0;
    vkEnumerateInstanceExtensionProperties(nullptr, &extCount, nullptr);
    std::vector<VkExtensionProperties> exts(extCount);
    vkEnumerateInstanceExtensionProperties(nullptr, &extCount, exts.data());

    auto hasExt = [&](const char* name){
        for (auto const& e : exts) if (strcmp(e.extensionName, name) == 0) return true;
        return false;
    };

    std::vector<const char*> enabledExts;
    if (enableDebugUtils && hasExt(VK_EXT_DEBUG_UTILS_EXTENSION_NAME)) {
        enabledExts.push_back(VK_EXT_DEBUG_UTILS_EXTENSION_NAME);
    }
    // portability enumeration is harmless if present (mostly for MoltenVK/macOS)
#ifdef VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME
    if (hasExt(VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME)) {
        enabledExts.push_back(VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME);
    }
#endif

    VkApplicationInfo appInfo{VK_STRUCTURE_TYPE_APPLICATION_INFO};
    appInfo.pApplicationName = "Multi-Framework Compute Client (Vulkan)";
    appInfo.applicationVersion = VK_MAKE_VERSION(1,0,0);
    appInfo.pEngineName = "No Engine";
    appInfo.engineVersion = VK_MAKE_VERSION(1,0,0);
    appInfo.apiVersion = VK_API_VERSION_1_1; // reasonable baseline for Linux/Windows in 2025

    VkInstanceCreateInfo ci{VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO};
    ci.pApplicationInfo = &appInfo;
    ci.enabledExtensionCount = static_cast<uint32_t>(enabledExts.size());
    ci.ppEnabledExtensionNames = enabledExts.empty() ? nullptr : enabledExts.data();
#ifdef VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME
    if (std::find(enabledExts.begin(), enabledExts.end(), VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME) != enabledExts.end()) {
        ci.flags |= VK_INSTANCE_CREATE_ENUMERATE_PORTABILITY_BIT_KHR;
    }
#endif

    VkResult res = vkCreateInstance(&ci, nullptr, &instance);
    if (res != VK_SUCCESS) {
        std::cerr << "vkCreateInstance failed: " << res << std::endl;
        return false;
    }
    return true;
}

bool VulkanExecutor::pickPhysicalDevice() {
    uint32_t count = 0;
    vkEnumeratePhysicalDevices(instance, &count, nullptr);
    if (count == 0) return false;
    std::vector<VkPhysicalDevice> devs(count);
    vkEnumeratePhysicalDevices(instance, &count, devs.data());

    // Prefer a device with a dedicated compute queue if possible
    auto pickScore = [&](VkPhysicalDevice pd)->int{
        VkPhysicalDeviceProperties props{};
        vkGetPhysicalDeviceProperties(pd, &props);
        uint32_t qfCount = 0;
        vkGetPhysicalDeviceQueueFamilyProperties(pd, &qfCount, nullptr);
        std::vector<VkQueueFamilyProperties> qf(qfCount);
        vkGetPhysicalDeviceQueueFamilyProperties(pd, &qfCount, qf.data());

        bool hasComputeOnly = false;
        bool hasCompute = false;
        for (uint32_t i=0;i<qfCount;i++){
            if (qf[i].queueFlags & VK_QUEUE_COMPUTE_BIT) {
                hasCompute = true;
                if ((qf[i].queueFlags & VK_QUEUE_GRAPHICS_BIT) == 0) hasComputeOnly = true;
            }
        }
        int score = 0;
        if (hasCompute) score += 10;
        if (hasComputeOnly) score += 5;
        // prefer discrete
        if (props.deviceType == VK_PHYSICAL_DEVICE_TYPE_DISCRETE_GPU) score += 3;
        return score;
    };

    int bestScore = -1;
    for (auto pd : devs) {
        int s = pickScore(pd);
        if (s > bestScore) { bestScore = s; physicalDevice = pd; }
    }
    if (physicalDevice == VK_NULL_HANDLE) return false;

    vkGetPhysicalDeviceProperties(physicalDevice, &deviceProperties);
    vkGetPhysicalDeviceMemoryProperties(physicalDevice, &memoryProperties);
    return true;
}

bool VulkanExecutor::createLogicalDevice() {
    uint32_t qfCount = 0;
    vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &qfCount, nullptr);
    std::vector<VkQueueFamilyProperties> qf(qfCount);
    vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &qfCount, qf.data());

    bool found = false;
    for (uint32_t i=0;i<qfCount;i++){
        if (qf[i].queueFlags & VK_QUEUE_COMPUTE_BIT) { computeQueueFamilyIndex = i; found = true; break; }
    }
    if (!found) return false;

    float prio = 1.0f;
    VkDeviceQueueCreateInfo qci{VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO};
    qci.queueFamilyIndex = computeQueueFamilyIndex;
    qci.queueCount = 1;
    qci.pQueuePriorities = &prio;

    VkPhysicalDeviceFeatures feats{}; // minimal
    VkDeviceCreateInfo dci{VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO};
    dci.queueCreateInfoCount = 1;
    dci.pQueueCreateInfos = &qci;
    dci.pEnabledFeatures = &feats;

    VK_CHECK(vkCreateDevice(physicalDevice, &dci, nullptr, &device));
    vkGetDeviceQueue(device, computeQueueFamilyIndex, 0, &computeQueue);

    // Pipeline cache (optional)
    VkPipelineCacheCreateInfo pci{VK_STRUCTURE_TYPE_PIPELINE_CACHE_CREATE_INFO};
    VK_CHECK(vkCreatePipelineCache(device, &pci, nullptr, &pipelineCache));
    return true;
}

bool VulkanExecutor::createCommandPool() {
    VkCommandPoolCreateInfo ci{VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO};
    ci.queueFamilyIndex = computeQueueFamilyIndex;
    ci.flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT;
    VK_CHECK(vkCreateCommandPool(device, &ci, nullptr, &commandPool));
    return true;
}

bool VulkanExecutor::createDescriptorPool() {
    // We assume at most 2 STORAGE_BUFFER per set (in/out). Size pool liberally.
    std::array<VkDescriptorPoolSize,1> sizes{};
    sizes[0].type = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
    sizes[0].descriptorCount = 2048; // 1000 sets * 2 buffers (rounded up)

    VkDescriptorPoolCreateInfo ci{VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO};
    ci.flags = VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT;
    ci.poolSizeCount = static_cast<uint32_t>(sizes.size());
    ci.pPoolSizes = sizes.data();
    ci.maxSets = 1024;

    VK_CHECK(vkCreateDescriptorPool(device, &ci, nullptr, &descriptorPool));
    return true;
}

void VulkanExecutor::destroyDeviceObjects() {
    if (descriptorPool) vkDestroyDescriptorPool(device, descriptorPool, nullptr);
    if (commandPool) vkDestroyCommandPool(device, commandPool, nullptr);
    if (pipelineCache) vkDestroyPipelineCache(device, pipelineCache, nullptr);
    if (device) vkDestroyDevice(device, nullptr);
    descriptorPool = VK_NULL_HANDLE;
    commandPool = VK_NULL_HANDLE;
    pipelineCache = VK_NULL_HANDLE;
    device = VK_NULL_HANDLE;
}

void VulkanExecutor::destroyInstance() {
    if (instance) vkDestroyInstance(instance, nullptr);
    instance = VK_NULL_HANDLE;
}

bool VulkanExecutor::initialize(const json& config) {
    if (initialized) return true;

    bool enableValidation = config.value("enableValidation", false);
    bool enableDebugUtils = config.value("enableDebugUtils", enableValidation);

    try {
        if (!createInstance(enableValidation, enableDebugUtils)) return false;
        if (!pickPhysicalDevice()) throw std::runtime_error("No suitable physical device");
        if (!createLogicalDevice()) throw std::runtime_error("Failed to create logical device");
        if (!createCommandPool()) throw std::runtime_error("Failed to create command pool");
        if (!createDescriptorPool()) throw std::runtime_error("Failed to create descriptor pool");
        initialized = true;
        std::cout << "Vulkan initialized: " << deviceProperties.deviceName << std::endl;
        return true;
    } catch (const std::exception& e) {
        std::cerr << "[initialize] " << e.what() << std::endl;
        // Clean up partially created resources
        destroyDeviceObjects();
        destroyInstance();
        initialized = false;
        return false;
    }
}

void VulkanExecutor::cleanup() {
    destroyDeviceObjects();
    destroyInstance();
    initialized = false;
}

// -------- Buffers --------
uint32_t VulkanExecutor::findMemoryType(uint32_t typeFilter, VkMemoryPropertyFlags properties) const {
    for (uint32_t i = 0; i < memoryProperties.memoryTypeCount; i++) {
        if ((typeFilter & (1u << i)) &&
            (memoryProperties.memoryTypes[i].propertyFlags & properties) == properties) {
            return i;
        }
    }
    throw std::runtime_error("No suitable memory type");
}

bool VulkanExecutor::createBuffer(VkDeviceSize size, VkBufferUsageFlags usage,
                                  VkMemoryPropertyFlags properties, VkBuffer& buffer, VkDeviceMemory& bufferMemory) const {
    VkBufferCreateInfo bi{VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO};
    bi.size = size;
    bi.usage = usage;
    bi.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
    VK_CHECK(vkCreateBuffer(device, &bi, nullptr, &buffer));

    VkMemoryRequirements req;
    vkGetBufferMemoryRequirements(device, buffer, &req);

    VkMemoryAllocateInfo ai{VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO};
    ai.allocationSize = req.size;
    ai.memoryTypeIndex = findMemoryType(req.memoryTypeBits, properties);

    VK_CHECK(vkAllocateMemory(device, &ai, nullptr, &bufferMemory));
    VK_CHECK(vkBindBufferMemory(device, buffer, bufferMemory, 0));
    return true;
}

// -------- Shader compilation --------
static shaderc_env_version pick_env_from_api(uint32_t apiVersion) {
    uint32_t major = VK_VERSION_MAJOR(apiVersion);
    uint32_t minor = VK_VERSION_MINOR(apiVersion);
    if (major > 1 || (major==1 && minor >= 2)) return shaderc_env_version_vulkan_1_2;
    if (minor >= 1) return shaderc_env_version_vulkan_1_1;
    return shaderc_env_version_vulkan_1_0;
}

bool VulkanExecutor::compileGLSLtoSPIRV(const std::string& glsl, std::vector<uint32_t>& spirv, std::string& error) const {
    shaderc::Compiler compiler;
    shaderc::CompileOptions options;
    options.SetTargetEnvironment(shaderc_target_env_vulkan, pick_env_from_api(deviceProperties.apiVersion));
    // Let drivers legalize. Optimize for performance if requested later.
    options.SetOptimizationLevel(shaderc_optimization_level_performance);

    auto res = compiler.CompileGlslToSpv(glsl, shaderc_compute_shader, "kernel.comp", options);
    if (res.GetCompilationStatus() != shaderc_compilation_status_success) {
        error = res.GetErrorMessage();
        return false;
    }
    spirv.assign(res.cbegin(), res.cend());
    return true;
}

// -------- Execute --------
TaskResult VulkanExecutor::executeTask(const TaskData& task) {
    TaskResult result;
    auto startTime = std::chrono::high_resolution_clock::now();

    if (!initialized) {
        result.success = false;
        result.errorMessage = "Vulkan not initialized";
        return result;
    }

    // Validate workgroup counts
    uint32_t gx = 1, gy = 1, gz = 1;
    if (!task.workgroupCount.empty()) {
        if (task.workgroupCount.size() > 0) gx = std::max(1, task.workgroupCount[0]);
        if (task.workgroupCount.size() > 1) gy = std::max(1, task.workgroupCount[1]);
        if (task.workgroupCount.size() > 2) gz = std::max(1, task.workgroupCount[2]);
    }
    // Clamp to device limits
    gx = std::min<uint32_t>(gx, deviceProperties.limits.maxComputeWorkGroupCount[0]);
    gy = std::min<uint32_t>(gy, deviceProperties.limits.maxComputeWorkGroupCount[1]);
    gz = std::min<uint32_t>(gz, deviceProperties.limits.maxComputeWorkGroupCount[2]);

    // Build descriptor set layout based on bindLayout string (server default: "vulkan-storage-buffer")
    // ABI: set=0, binding 0 -> optional readonly storage buffer (input)
    //      set=0, binding 1 -> storage buffer (output)
    VkDescriptorSetLayoutBinding bindings[2]{};
    uint32_t bindingCount = 0;

    bool hasInput = !task.inputData.empty();
    // Always provide output
    if (hasInput) {
        bindings[bindingCount].binding = 0;
        bindings[bindingCount].descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
        bindings[bindingCount].descriptorCount = 1;
        bindings[bindingCount].stageFlags = VK_SHADER_STAGE_COMPUTE_BIT;
        bindingCount++;
    }
    bindings[bindingCount].binding = hasInput ? 1u : 0u;
    bindings[bindingCount].descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
    bindings[bindingCount].descriptorCount = 1;
    bindings[bindingCount].stageFlags = VK_SHADER_STAGE_COMPUTE_BIT;
    bindingCount++;

    VkDescriptorSetLayoutCreateInfo dslci{VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO};
    dslci.bindingCount = bindingCount;
    dslci.pBindings = bindings;
    VkDescriptorSetLayout descriptorSetLayout = VK_NULL_HANDLE;
    try {
        VK_CHECK(vkCreateDescriptorSetLayout(device, &dslci, nullptr, &descriptorSetLayout));
    } catch (...) {
        result.success = false;
        result.errorMessage = "Failed to create descriptor set layout";
        return result;
    }

    // Pipeline layout
    VkPipelineLayoutCreateInfo plci{VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO};
    plci.setLayoutCount = 1;
    plci.pSetLayouts = &descriptorSetLayout;
    VkPipelineLayout pipelineLayout = VK_NULL_HANDLE;
    VkPipeline pipeline = VK_NULL_HANDLE;
    VkShaderModule shaderModule = VK_NULL_HANDLE;

    // Resources
    VkBuffer inputBuffer = VK_NULL_HANDLE, outputBuffer = VK_NULL_HANDLE;
    VkDeviceMemory inputMem = VK_NULL_HANDLE, outputMem = VK_NULL_HANDLE;
    VkDescriptorSet descriptorSet = VK_NULL_HANDLE;
    VkCommandBuffer cmd = VK_NULL_HANDLE;
    VkFence fence = VK_NULL_HANDLE;

    try {
        VK_CHECK(vkCreatePipelineLayout(device, &plci, nullptr, &pipelineLayout));

        // Compile GLSL (we only support GLSL text for Vulkan client)
        std::vector<uint32_t> spirv;
        std::string compileErr;
        if (!compileGLSLtoSPIRV(task.kernel, spirv, compileErr)) {
            throw std::runtime_error(std::string("GLSL->SPIR-V compile failed: ") + compileErr);
        }

        VkShaderModuleCreateInfo smci{VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO};
        smci.codeSize = spirv.size() * sizeof(uint32_t);
        smci.pCode = spirv.data();
        VK_CHECK(vkCreateShaderModule(device, &smci, nullptr, &shaderModule));

        VkPipelineShaderStageCreateInfo stage{VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO};
        stage.stage = VK_SHADER_STAGE_COMPUTE_BIT;
        stage.module = shaderModule;
        stage.pName = task.entry.empty() ? "main" : task.entry.c_str();

        VkComputePipelineCreateInfo cpci{VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO};
        cpci.stage = stage;
        cpci.layout = pipelineLayout;
        VK_CHECK(vkCreateComputePipelines(device, pipelineCache, 1, &cpci, nullptr, &pipeline));

        // Buffers
        if (hasInput) {
            if(!createBuffer(task.inputData.size(),
                                  VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT,
                                  VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
                                  inputBuffer, inputMem)) {
                                    throw std::runtime_error("Failed to create input buffer");
                                  }
            // Map & write input
            void* p = nullptr;
            VkResult mapRes = vkMapMemory(device, inputMem, 0, task.inputData.size(), 0, &p);
            if (mapRes != VK_SUCCESS || !p) throw std::runtime_error("vkMapMemory failed for input buffer");
            std::memcpy(p, task.inputData.data(), task.inputData.size());
            vkUnmapMemory(device, inputMem);
        }

        if (task.outputSize == 0) throw std::runtime_error("outputSize must be > 0");
        if (!createBuffer(task.outputSize,
                  VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
                  VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
                  outputBuffer, outputMem)) {
                throw std::runtime_error("Failed to create output buffer");
            }

        // Allocate descriptor set
        VkDescriptorSetLayout layouts[1] = { descriptorSetLayout };
        VkDescriptorSetAllocateInfo dsai{VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO};
        dsai.descriptorPool = descriptorPool;
        dsai.descriptorSetCount = 1;
        dsai.pSetLayouts = layouts;
        VK_CHECK(vkAllocateDescriptorSets(device, &dsai, &descriptorSet));

        // Update descriptors
        std::vector<VkWriteDescriptorSet> writes;
        VkDescriptorBufferInfo inInfo{}, outInfo{};
        if (hasInput) {
            inInfo.buffer = inputBuffer; inInfo.offset = 0; inInfo.range = task.inputData.size();
            VkWriteDescriptorSet w{VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET};
            w.dstSet = descriptorSet;
            w.dstBinding = 0;
            w.descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
            w.descriptorCount = 1;
            w.pBufferInfo = &inInfo;
            writes.push_back(w);
        }
        outInfo.buffer = outputBuffer; outInfo.offset = 0; outInfo.range = task.outputSize;
        {
            VkWriteDescriptorSet w{VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET};
            w.dstSet = descriptorSet;
            w.dstBinding = hasInput ? 1u : 0u;
            w.descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
            w.descriptorCount = 1;
            w.pBufferInfo = &outInfo;
            writes.push_back(w);
        }
        vkUpdateDescriptorSets(device, static_cast<uint32_t>(writes.size()), writes.data(), 0, nullptr);

        // Command buffer
        VkCommandBufferAllocateInfo cbai{VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO};
        cbai.commandPool = commandPool;
        cbai.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
        cbai.commandBufferCount = 1;
        VK_CHECK(vkAllocateCommandBuffers(device, &cbai, &cmd));

        VkCommandBufferBeginInfo beginInfo{VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO};
        VK_CHECK(vkBeginCommandBuffer(cmd, &beginInfo));

        // Bind pipeline & descriptors
        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
        vkCmdBindDescriptorSets(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipelineLayout, 0, 1, &descriptorSet, 0, nullptr);

        // Dispatch
        vkCmdDispatch(cmd, gx, gy, gz);

        // Barrier to make shader writes visible to host (explicit even if we wait on fence & coherent memory)
        VkMemoryBarrier mb{VK_STRUCTURE_TYPE_MEMORY_BARRIER};
        mb.srcAccessMask = VK_ACCESS_SHADER_WRITE_BIT;
        mb.dstAccessMask = VK_ACCESS_HOST_READ_BIT;
        vkCmdPipelineBarrier(cmd,
                             VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                             VK_PIPELINE_STAGE_HOST_BIT,
                             0,
                             1, &mb,
                             0, nullptr,
                             0, nullptr);

        VK_CHECK(vkEndCommandBuffer(cmd));

        // Fence & submit
        VkFenceCreateInfo fci{VK_STRUCTURE_TYPE_FENCE_CREATE_INFO};
        VK_CHECK(vkCreateFence(device, &fci, nullptr, &fence));

        VkSubmitInfo si{VK_STRUCTURE_TYPE_SUBMIT_INFO};
        si.commandBufferCount = 1;
        si.pCommandBuffers = &cmd;
        VK_CHECK(vkQueueSubmit(computeQueue, 1, &si, fence));
        VK_CHECK(vkWaitForFences(device, 1, &fence, VK_TRUE, UINT64_C(30'000'000'000))); // 30s

        // Read back output
        void* mapped = nullptr;
        VkResult mapRes = vkMapMemory(device, outputMem, 0, task.outputSize, 0, &mapped);
        if (mapRes != VK_SUCCESS || !mapped) throw std::runtime_error("vkMapMemory failed for output buffer");
        auto* b = static_cast<uint8_t*>(mapped);
        result.outputData.assign(b, b + task.outputSize);
        vkUnmapMemory(device, outputMem);

        result.success = true;
        auto end = std::chrono::high_resolution_clock::now();
        // TODO: fix time measurement result.computeTimeMs = std::chrono::duration_cast<std::chrono::milliseconds>(end - startTime).count();
        result.errorMessage.clear();
    } catch (const std::exception& e) {
        result.success = false;
        result.errorMessage = e.what();
    }

    // Cleanup per-task allocations
    if (fence) vkDestroyFence(device, fence, nullptr);
    if (cmd) vkFreeCommandBuffers(device, commandPool, 1, &cmd);
    if (descriptorSet) vkFreeDescriptorSets(device, descriptorPool, 1, &descriptorSet);
    if (shaderModule) vkDestroyShaderModule(device, shaderModule, nullptr);
    if (pipeline) vkDestroyPipeline(device, pipeline, nullptr);
    if (pipelineLayout) vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
    if (descriptorSetLayout) vkDestroyDescriptorSetLayout(device, descriptorSetLayout, nullptr);
    if (inputBuffer) vkDestroyBuffer(device, inputBuffer, nullptr);
    if (inputMem) vkFreeMemory(device, inputMem, nullptr);
    if (outputBuffer) vkDestroyBuffer(device, outputBuffer, nullptr);
    if (outputMem) vkFreeMemory(device, outputMem, nullptr);

    return result;
}

json VulkanExecutor::getCapabilities() const {
    json caps;
    caps["framework"] = "vulkan";
    caps["initialized"] = initialized;
    if (!initialized || physicalDevice == VK_NULL_HANDLE) return caps;

    caps["device"] = {
        {"name", deviceProperties.deviceName},
        {"apiVersion", deviceProperties.apiVersion},
        {"driverVersion", deviceProperties.driverVersion},
        {"vendorID", deviceProperties.vendorID},
        {"deviceID", deviceProperties.deviceID},
        {"type", deviceProperties.deviceType}
    };
    caps["limits"] = {
        {"maxComputeWorkGroupCount", {
            deviceProperties.limits.maxComputeWorkGroupCount[0],
            deviceProperties.limits.maxComputeWorkGroupCount[1],
            deviceProperties.limits.maxComputeWorkGroupCount[2]
        }},
        {"maxComputeWorkGroupSize", {
            deviceProperties.limits.maxComputeWorkGroupSize[0],
            deviceProperties.limits.maxComputeWorkGroupSize[1],
            deviceProperties.limits.maxComputeWorkGroupSize[2]
        }},
        {"maxComputeWorkGroupInvocations", deviceProperties.limits.maxComputeWorkGroupInvocations}
    };
    return caps;
}
./vulkan/vulkan_executor.hpp
#pragma once
#include "../common/framework_client.hpp"
#include <vulkan/vulkan.h>
#include <vector>
#include <memory>

// NOTE: This header replaces the previous include path "../common/framework_client.hpp".
// If your tree uses a different layout, adjust the include accordingly.

class VulkanExecutor : public IFrameworkExecutor {
private:
    VkInstance instance = VK_NULL_HANDLE;
    VkPhysicalDevice physicalDevice = VK_NULL_HANDLE;
    VkDevice device = VK_NULL_HANDLE;
    VkQueue computeQueue = VK_NULL_HANDLE;
    VkCommandPool commandPool = VK_NULL_HANDLE;
    VkDescriptorPool descriptorPool = VK_NULL_HANDLE;
    VkPipelineCache pipelineCache = VK_NULL_HANDLE;
    uint32_t computeQueueFamilyIndex = 0;
    bool initialized = false;

    // Cached device properties/memory properties
    VkPhysicalDeviceProperties deviceProperties{};
    VkPhysicalDeviceMemoryProperties memoryProperties{};

    // Helpers
    bool createInstance(bool enableValidation, bool enableDebugUtils);
    bool pickPhysicalDevice();
    bool createLogicalDevice();
    bool createCommandPool();
    bool createDescriptorPool();
    void destroyInstance();
    void destroyDeviceObjects();

    // Shader compilation
    bool compileGLSLtoSPIRV(const std::string& glsl, std::vector<uint32_t>& spirv, std::string& error) const;

    // Buffer helpers
    uint32_t findMemoryType(uint32_t typeFilter, VkMemoryPropertyFlags properties) const;
    bool createBuffer(VkDeviceSize size, VkBufferUsageFlags usage,
                      VkMemoryPropertyFlags properties, VkBuffer& buffer, VkDeviceMemory& bufferMemory) const;

public:
    VulkanExecutor();
    ~VulkanExecutor() override;

    bool initialize(const json& config = {}) override;
    void cleanup() override;
    TaskResult executeTask(const TaskData& task) override;
    std::string getFrameworkName() const override { return "vulkan"; }
    json getCapabilities() const override;
};
